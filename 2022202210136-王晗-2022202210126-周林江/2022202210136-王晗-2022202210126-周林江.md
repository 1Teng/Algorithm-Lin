# 算法实践题

**王晗 2022202210136**

**周林江 2022202210126**

---

[TOC]

---

## 问题说明

装箱问题是一个典型的优化问题，题目要求三维数据，但是不要求重量和价值，这就是three-dimensional bin-packing problem。但是题目限制了车厢的数量为1，所以是从bin-packing问题变成了其中的一个特例——Geometric knapsack problem，几何背包问题，只不过所有的物品都是立方体。

---

## 离线算法——模拟退火

### 运行

如需运行，将数据文件置于`3d-bpp-main-2`主文件夹下

数据文件格式如下

- 每两行为一个样例输入
- 样例输入的数量不限
- 文件中不能有无数据的单独空行
- 文件中不能有中文字符
- 以老师的数据为例，需要先删除所有的注释和空行，并将其中的中文逗号替换为英文

```
C (587 233 220)
B [(108 76 30 40), (110 43 25 33), (92 81 55 39)]
C (587 233 220)
B [(91 54 45 32), (105 77 72 24), (79 78 48 30)]
```

然后选择`src`文件夹下的`test2.ipynb`，修改第四个cell中的`while index < 50:`中的50为数据文件的具体行数。然后顺序运行该文件即可。

一个样例输出如下所示，`original ratio (without SA) is  0.6412323253002198`表示了未修改的原始结果，最后的`0.8091667491978961`表示使用了模拟退火的结果。

```
**************************
case:  7
original ratio (without SA) is  0.6412323253002198
0 0.713968908674548 0.539037415560582
0 0.8885793395766907 0.6680124242180526
1 0.6056102017241954 0.7430474695260358
1 0.6056102017241954 0.7430474695260358
8 0.8306613420935129 0.6093662199788499
10 0.9128919633272922 0.8091667491978961
Bin(LayerPool(layers=[Layer(height=36, ids=[3, 12, 18, 19, 20, 29, 31, 34, 37, 50, 52, 54, 57, 60, 65, 66, 67, 69, 80, 85, 93, 103, 108, 109]), Layer(height=39, ids=[5, 25, 40, 41, 49, 53, 55, 56, 61, 68, 70, 74, 79, 84, 86, 89, 91, 92, 94, 101, 102, 106, 115, 116]), Layer(height=72, ids=[27, 36, 39, 42, 43, 45, 46, 47, 48, 51, 58, 59, 62, 63, 64, 71, 72, 77]), Layer(height=43, ids=[0, 14, 15, 17, 21, 23, 28, 32, 35, 38, 44, 76, 112])])) 0.6524538030058206 0.8091667491978961
**************************
```

如果无法运行，请先检查依赖库是否安装完全。

### 算法思想

基础接口部分是对两个库进行了修改，分别是3d装箱问题的`3d-bpp`库和2d装箱问题的`rectpack`库。

`rectpack` 是用于解决 2D 背包问题（也称为装箱问题）的启发式算法的集合。本质上是将一组矩形打包到最少数量的箱子中。如下图所示：

<img src="https://github.com/secnot/rectpack/raw/master/docs/maxrects.png" alt="替代标签" style="zoom:50%;" />

如何将2d的装箱问题移植到3d装箱上呢？`3d-bpp`实现了其中一部分，但是有所缺陷，我们进行了改进。

在`3d-bpp`的解决方案中，由于使用了层和高度组，`rectpack`中的`maxrects`算法 被推广到 3D。特别是，项目根据其第三维（高度）和指定的高度差进行分组。然后将此类项目组用作 `maxrects` 过程的输入，在该过程中，图层（layer）被视为2d的箱子。

但是作者出现了两个小疏忽，

1. 可能出现空放的情况，也就是有箱子悬空了，这是因为作者没有处理好层与层之间的高度差。
2. 没有考虑到箱子旋转的情况。

以上两个问题造成了装箱算法无法落实（因为悬空），也造成了空间浪费的问题。

我们重构了作者的`layer`类，确保箱子能够叠放。同时利用模拟退火算法，旋转一部分的箱子，这从实验结果中可以看出，确实提高了空间利用率。

未来的可能改进：

1. 该算法在箱子高度差较多的时候结果不佳。因为需要处理过多的高度层。
2. 使用更好的硬件，离线部分在8750H上运行，所以模拟退火算法的参数都设置的比较小，在几组实验中`T_min`和`k`的取值确实会影响最后的空间占比。

### 部分实现

**利用接口进行计算**

`convert.convert`函数根据`flip_ratio`的大小调整箱子旋转的数量，并读取样例输入。之后准备数据，并将数据进行处理，所有的物品都会被装箱，从若干个参数相同的车厢中选出体积占比最大的，并返回。

```python
def calculate(flip_ratio):
    con=convert.convert(flip_ratio)
    product_dataset = dataset.ProductDataset( "../data",con[3],1,con[1],1,con[0],1,con[2],1,1000000,force_overload=2,)

    order = product_dataset.get_order(con[3]+1)
    bin_pool = main.main(
        order,
        procedure="mr",
    )
    maxn=0
    for i in bin_pool.get_original_bin_pool():
        if i.volume_ratio()>maxn:
            maxn=i.volume_ratio()
            id=i
    return id, maxn
```

**模拟退火部分**

首先设置包括起始温度，最低温度在内的多个参数。需要注意的是为了确保运行速度，没有将各种参数设置得很大，一个样例输入最多调用500次`calculate`函数，所以完整运行一个样例的时间大概在3分钟左右。

其中模拟退火部分主要如下，如果新生成的`x`在范围内，那么调用`calculate`函数，如果大于前一个`ratio`或者随机数`r`小于`p`，则接受这个值作为新的`x`，否则就拒绝。

同时因为迭代次数有限，不能很好的收敛，所以记录了每次运行的最大值。

```python
def SA():
    T=1000 #initiate temperature
    Tmin=20 #minimum value of terperature
    x=np.random.uniform(low=0,high=1)#initiate x
    k=5 #times of internal circulation
    t=0#time

    answer,maxn=calculate(0)
    ori_ans,ori_maxn=answer,maxn
    print("original ratio (without SA) is ", ori_maxn)
    while T>=Tmin:
        for i in range(k):
            id,ratio=calculate(x)
            if ratio > maxn:
                print(t,x,ratio)
                maxn=ratio
                answer=id
            xNew=x+np.random.uniform(low=-0.055,high=0.055)*T/100
            if (0<=xNew and xNew<=1):
                id_new,ratio_new=calculate(xNew)

                if ratio_new > maxn:
                    print(t,x,ratio)
                    maxn=ratio_new
                    answer=id_new

                if ratio_new>ratio:
                    x=xNew
                    id=id_new
                else:
                    p=math.exp(-(ratio_new-ratio)/T*100)
                    r=np.random.uniform(low=0,high=1)
                    if r<p:
                        x=xNew
        t+=1

        T=1000/(1+t)
    answer.plot()
    print(ori_maxn,ratio,maxn)
    return ori_ans,ori_maxn,answer,maxn
```

**主函数**

处理整个输入文件，输入文件中包含25组样例输入，最后输出最佳空间占比，并将结果写入`result`文件夹下的csv文件中。

```python
index = 0
while index < 50:
    print("**************************")
    print("case: ", int(index/2))
    convert.pre_convert(index)
    ori_ans,ori_maxn,answer,maxn=SA()
    answer.to_dataframe().to_csv("../result/case"+str(int(index/2))+".csv")
    index += 2
    print("**************************")
```

### 实验结果

装箱体积占比如下，原始结果指的是不进行任何旋转的体积占比，模拟退火结果指的是选中若干箱子进行旋转后的体积占比。具体的箱子摆放位置在`result`文件夹下的`csv`文件中。

采用了模拟退火对箱子进行旋转后，体积占比明显上升了，体现了我们改进的有效性。同时也可以看到高度差异太多之后，空间占比会有所下降。

| 组数  | 原始结果               | 模拟退火结果             |
| --- | ------------------ | ------------------ |
| 0   | 0.6589847262943168 | 0.7307513355103853 |
| 1   | 0.664420753735009  | 0.7418055794656098 |
| 2   | 0.6447610837225595 | 0.7364707164796365 |
| 3   | 0.5109187154905911 | 0.6951084792695953 |
| 4   | 0.6705154136210427 | 0.6990423275534885 |
| 5   | 0.5577019583497564 | 0.7199373072840402 |
| 6   | 0.5752534594986577 | 0.7841331994222592 |
| 7   | 0.6412323253002198 | 0.8091667491978961 |
| 8   | 0.6504871779703433 | 0.702611864157806  |
| 9   | 0.5357577795930956 | 0.732958674785524  |
| 10  | 0.6000315058814302 | 0.696623752642938  |
| 11  | 0.6501057839879666 | 0.7548000938529632 |
| 12  | 0.5525106664690348 | 0.7400899047578534 |
| 13  | 0.5039549851410553 | 0.7532424802971922 |
| 14  | 0.6001895337993634 | 0.7164641161968812 |
| 15  | 0.5540932387979642 | 0.6877102469223606 |
| 16  | 0.578156055144598  | 0.628689162575001  |
| 17  | 0.5999286132559999 | 0.6659033912691487 |
| 18  | 0.5703058729222902 | 0.700166436133125  |
| 19  | 0.5564129424033936 | 0.6704981983820334 |
| 20  | 0.5753933083900694 | 0.6732521048786924 |
| 21  | 0.523311161789348  | 0.695063347426787  |
| 22  | 0.4446161500211701 | 0.6239031267260936 |
| 23  | 0.3918378497302392 | 0.6244320466659267 |
| 24  | 0.5966243176218244 | 0.7656164152289062 |

以第24组为例，摆放可视化如下，具体的可视化图片在src文件夹下的`test2.ipynb`文件中。

![image-20221229163240368](https://kw-20200521.oss-cn-beijing.aliyuncs.com/img/image-20221229163240368.png)

---

## 在线算法——Deep Q Network

为了构建在线算法，需要重新定义算法的运行环境。我们计划使用强化学习中的DQN（Deep Q Network）算法来构建算法模型。而由于强化学习特殊的交互要求，我们重新构建了一个基于OpenAI开源的强化学习环境库GYM的强化学习环境。然后基于此环境，构建了一个DQN模型进行训练，并测试其性能。

### 强化学习环境

一般而言，强化学习的基本模式如下图所示。强化学习的目的在于通过与环境（Environmnet）进行交互训练一个智能体（Agent）使得智能体能够取得尽可能大的奖励（Reward）。强化学习智能体与环境的基本交互模式为：智能体通过观察（observation）环境得到环境的状态（State），然后进行决策做出相应的动作（Action），最后环境根据智能体的动作模拟计算出相应的奖励（Reward）。

![image-20221230200611046](C:/Users/13103/AppData/Roaming/Typora/typora-user-images/image-20221230200611046.png)

为了构建强化学习环境，我们构建了如下的GYM框架。

#### 环境初始化

环境依据箱子的大小初始化容器的信息，然后根据箱子大小和输入的物品数目随机生成相应数目的物品。环境初始化后，输出最初的环境状态。

#### 环境交互

环境的状态包含如下三个信息：

1. 箱子的高度俯视图$heightmap$
2. 要装入箱子的物体
3. 物体可以放入的位置环境

环境接收的动作为一个整数值，表明了箱子将要放置的位置。

#### 动作空间

整数值（$0\le a\le n$, $n$为$heightmap$的大小），表明了箱子将要放置的位置。

#### 环境奖励

环境奖励参考[1]设置为物体的体积。显然，当一个episode交互完毕，奖励的总和就代表着放入箱子的物体的总体积，这与算法设计的目标是一致的。

### Deep Q Network算法

强化学习的目标为：训练智能体找到一个策略$\pi(a|s):(S\to A)$，其中$S$代表状态空间，$A$代表动作空间，$a\in A$,$s\in S$，通过策略生成一系列的智能体行为$a_i$，最大化动作序列的奖励期望$E_a(R^a_s)$。

#### 马尔可夫决策过程

一般来说，使用价值函数$V_\pi(s)$或$Q_\pi(s,a)$来表示一个状态的期望奖励。对于强化学习模型，一般将其建模为马尔可夫决策过程。一个马尔可夫决策过程由元组$<S,A,P,r,\gamma>$组成。

- $S$为状态集合；
- $A$为动作集合；
- $\gamma$为折扣因子；
- $r(s,a)$为奖励函数，反映了在状态$s$下，动作$a$对应的奖励；
- $P(s'|s,a)$为状态转移函数，表示状态$s$下，执行动作$a$之后，转移为状态$s'$的概率。

两个价值函数的贝尔曼期望方程为：
$$
V^\pi(s)=\sum_{a\in A}\pi(a|s)(r(s,a)+\gamma \sum_{s'\in S}p(s'|s,a)V^\pi(s'))
$$

$$
Q^\pi(s,a)=r(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)\sum_{a'\in A}\pi(a'|s')Q^\pi(s',a')
$$

#### Q-learning 算法

Q-learning算法是一种估计最优动作值函数$Q_\pi^*(s,a)$的强化学习算法。动作值函数的贝尔曼最优方程如下：
$$
Q^*(s,a)=r(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)\max_{a'}Q*(s',a')
$$
通过使用$\epsilon$-贪心策略就可以由最优动作值函数$Q_\pi^*(s,a)$得到策略算法。

为了估计优动作值函数$Q_\pi^*(s,a)$，Q-learning算法使用时序差分的方式来更新Q函数。
$$
Q(s_t,a_t)\gets Q(s_t,a_t)+\alpha[R_t+\gamma \max_aQ(s_{t+1},a)-Q(s_t,a_t)]
$$
Q算法的具体流程如下：

- 初始化$Q(s,a)$
- for 序列$e=1\to E$ do:
- ​    得到初始状态$s$
- ​    for 时间步$t=1\to T$ do:
- ​        用$\epsilon$-贪心策略根据Q选择当前状态$s$下的动作$a$
- ​        得到环境反馈的$r$和$s'$
- ​        更新$Q(s,a)\gets Q(s,a)+\alpha[R_t+\gamma \max_{a'}Q(s',a')-Q(s,a)]$
- ​        $s\gets s'$
- ​    end for
- end for

#### Deep Q Network

在Q-learning算法中，Q值是以矩阵的形式存储起来的，但是当动作空间和状态空间很大时，存储Q值就几乎是不现实的。因此需要用函数拟合的方式来估计Q-learning算法中的Q函数。Deep Q Network（DQN）就是使用深度神经网络来拟合Q函数的算法。

DQN算法构建了一个神经网络$Q_w(s,a)$，其误差函数使用均方误差可以表示为：
$$
w^*=\arg\min_w\frac{1}{2N}\sum^N_{i=1}[Q_w(s_i,a_i)-(r_i+\gamma \max_{a'}Q_w(s_i',a'))]^2
$$
为了更好地训练和使用DQN网络，DQN还包含了经验回放和目标网络机制。

在原来的Q-learning算法中，每一个数据只会用来更新一次$Q$值。为了更好地将Q-learning 和深度神经网络结合，DQN算法采用了经验回放方法，具体做法为维护一个回放缓冲区，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练Q网络的时候再从回放缓冲区中随机采样若干数据来进行训练。

DQN 算法最终更新的目标是逼近Q函数的值，由于误差目标本身就包含神经网络的输出，因此在更新网络参数的同时目标也在不断地改变，这非常容易造成神经网络训练的不稳定性。为了解决这一问题，DQN便使用了目标网络的思想：训练过程中Q网络的不断更新会导致目标不断发生改变，所以暂时先将目标中的Q网络固定住。

DQN算法的具体流程如下：

- 初始化网络$Q_w(s,a)$
- 复制网络$Q_w$得到目标网络$Q_{w'}$
- 初始化经验回放池$R$
- for $e=1\to E$ do
- ​    获取环境初始状态$s_1$
- ​    for $t=1\to T$ do
- ​        根据当前网络$Q_w(s,a)$以$\epsilon$-贪心策略选择动作$a_t$
- ​        执行动作$a_t$，获得奖励$r_t$，环境转移为$s_{t+1}$
- ​        将$(s_t,a_t,r_t,s_{t+1})$存储到回放池$R$中
- ​        从$R$中采样$N$个数据$\{(s_t,a_t,r_t,s_{t+1})\}_{i=1,...,N}$
- ​        用目标网络计算$y_i=r_i+\gamma \max_a Q_{w'}(s_{i+1},a)$
- ​        最小化目标损失$L=\frac{1}{N}\sum_i(y_i-Q_w(s_i,a_i))^2$，更新$Q_w$
- ​        更新目标网络
- ​    end for
- end for



### 算法实验

#### 参数设置

环境参数

| 箱子大小     | 物品个数 | 可处理物品个数 | Episode |
| ------------ | -------- | -------------- | ------- |
| [10, 10, 10] | 128      | 1              | 1000    |

算法参数

| 折扣因子$\gamma$ | $\epsilon$消减因子 | MLP模型大小   | Batch Size |
| ---------------- | ------------------ | ------------- | ---------- |
| 0.95             | 0.99               | [128,256,128] | 4          |

#### 实验结果

模型在训练过程中的空间利用率随Episode变化图如下。

![scores](C:/Users/13103/Documents/GitHub/3D-BPP-RL/scores.png)

模型在整个交互过程中最高的空间利用率为$71.7%$。

#### 结论

在2017年，Bello等人[2]提出了基于强化学习的组合优化方法，从此，强化学习方法开始被大量应用在各种组合优化问题上。2017年，Hu等人[3]提出了基于策略梯度算法的3D-BPP问题的强化学习解决方法。在2021年，阿里巴巴菜鸟物流部门结合菜鸟物流的真实数据，提出了基于带约束的Actor-Critic算法的3D-BPP问题的强化学习解决方法[1]。随即在2022年，阿里巴巴再次优化了之前地强化学习解决方法，提出了基于树的强化学习方法[4]。由此可见，该问题是一个目前强化学习正在研究攻克的难题。本次我们使用的DQN算法是对强化学习解决该问题的一种探索。由于实验时间和实验条件的限制，我们暂时不能做到同论文[1]和论文[4]中的效果，但是也实现了一定的实验效果，证明了强化学习算法在解决该问题上可行性。

## 参考文献

[1] Zhao, H., Q. She, C. Zhu, Y. Yang, and K. Xu. “Online 3D Bin Packing With Constrained Deep Reinforcement Learning”. *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 35, no. 1, May 2021, pp. 741-9, doi:10.1609/aaai.v35i1.16155.

[2] Bello, Irwan et al. “Neural Combinatorial Optimization with Reinforcement Learning.” *ArXiv* abs/1611.09940 (2016): n. pag.

[3] Hu, Haoyuan et al. “Solving a New 3D Bin Packing Problem with Deep Reinforcement Learning Method.” *ArXiv* abs/1708.05930 (2017): n. pag.

[4] Zhao, Han et al. “Learning Efficient Online 3D Bin Packing on Packing Configuration Trees.” *International Conference on Learning Representations* (2022).
